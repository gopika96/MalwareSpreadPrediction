import request
from urllib2 import urlopen
from bs4 import BeautifulSoup
import re 
import csv

#----------Base URL ----------------
url_arr=["https://krebsonsecurity.com/"]
times =146
while (times > 0):
	html = urlopen(url_arr[-1])
	bsObj = BeautifulSoup(html,"lxml")

#------------creating a list of article specific URLs------------------

	for link in bsObj.find("div",{"id":"content"}).findAll("a", href =re.compile('https://krebsonsecurity.com/(?:[a-zA-Z]|[0-9]|[-_.+])+')):
		if 'href' in link.attrs:
			if link.attrs['href'] not in url_arr:
				new_url = link.attrs['href']
				print(new_url)
				url_arr.append(new_url)

	times = times -1

article_ls=[]

#---------------------Visiting each URL and extracting the data----------------

for ele in url_arr:
	result = re.match('https://krebsonsecurity.com/20(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',ele)
	if result:
		htm = urlopen(ele)
		soup = BeautifulSoup(htm,"lxml")
		essay=soup.find('div', class_="entry");
		article =essay.get_text()
		article_ls.append(article)
import pandas as pd
df = pd.DataFrame()
df = df.append(article_ls)
df.to_csv('krebs.csv',encoding='utf-8')
		
	
	


