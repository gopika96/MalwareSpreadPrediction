
# coding: utf-8

# ## Extracting just the links from the Security home page



base_url = "http://www.zdnet.com"
additional_url = "/topics/security/1150/"

import re
import numpy as np
import requests
from bs4 import BeautifulSoup
from time import sleep
from urllib2 import urlopen

# To keep a count of the number of articles to be scrapped
limit =1151;

next_page = base_url + additional_url

# List to store the links
list_of_links = []

# Change the limit as per requirements
while next_page and limit <=1300:
    
    temp_list_of_links = []
    print(next_page)
    print(limit)	
    # Load and extract the content of the page
    try:
        htm = urlopen(next_page)
        soup = BeautifulSoup(htm,"lxml")
    except:
        
        print("not reachable")
    # Find the 'news' links of the page
    
    for link in soup.find("section",{"id":"articleRiver"}).find_all('a', href=True):
        if link['href'].startswith('/article/'):
            temp_list_of_links.append(link['href'])
            
    # Save the unique links
    link_list = set(temp_list_of_links)
    
    # Find the length of the list of unique links
    length = len(link_list)
    #print(length)
    
    # Add the links to the final list
    list_of_links.extend(link_list)
    
    # Increment the limit
    
    
    # Find the links of the Show More page
    
    next_url = "/topics/security/"+str(limit)+"/"
    # Change the href to the Show More page link
    
    next_page = base_url + next_url
    limit = limit + 1   

    

# In[127]:

# Final list with unique links
link_list = set(list_of_links)

# Remove the lone '/news'/ link


# Converting the set into a list
link_list = list(link_list)


# ## Extracting the data from each link

# In[128]:
all_dates = []
all_articles = []
for item in link_list:
    
    new_page = base_url + item
    #print(new_page)
    page = requests.get(new_page)
    soup = BeautifulSoup(page.content, 'html.parser')
    d = []
    article = []
   
    

    #print(soup.prettify())

    article_content = []
    date = []
    
    content = soup.find("div", {"class":"storyBody"}).findAll('p')
	

    # Writing the content found in the list in its text form
    try:
        c1 = soup.find('time', datetime= True)
        date.append(c1['datetime'])
    except:
        print("There's no date")
    	
    	
    	
    for item in content:
    	article_content.append(item.text)

    # Joining the list elements to form a proper paragraph
    date = " ".join(date)
    d.append(date)
    all_dates.append(d)  


    article_content = " ".join(article_content)

    article.append(article_content)
    article.append(date)
    all_articles.append(article)




import pandas as pd
df = pd.DataFrame()
df = df.append(all_articles)
df.to_csv('final2.csv',mode = 'w', encoding='utf-8')





