from nltk import word_tokenize
import collections
import nltk
import csv
import string
from gensim.models import Phrases
from gensim.models import Word2Vec
from nltk.corpus import stopwords
import numpy as np
import sys
reload(sys)
sys.setdefaultencoding("utf-8")
stop_words =set(stopwords.words("english"))
 
sentences = []
bigram = Phrases()
bigram2 = Phrases()

#----Reading CSV--------------
with open("train_data.csv", "r") as sentencesfile:
	reader = csv.reader(sentencesfile, delimiter = ",")
	reader.next()
	for row in reader:
		sentence = []
		text=row[1].lower()
		text = text.encode('ascii', 'ignore').decode('ascii')
		words= word_tokenize(text)
		#-------------Word tokenization and stop words removal
		for word in words:
			if word not in stop_words and len(word)>3:
				sentence.append(word)
        
		sentences.append(sentence)
		bigram.add_vocab([sentence])
        
        
new_arr = []   
#------Bigram model    
bigram_model = Word2Vec(bigram[sentences], size=50)
bigram_model_counter = collections.Counter()
for key in bigram_model.wv.vocab.keys():
	if key not in stopwords.words("english"):
		if len(key.split("_")) > 1:
			new_arr.append(key)
			bigram.add_vocab([key])
			bigram_model_counter[key] += bigram_model.wv.vocab[key].count
#Output file
outfile = open("bigrams_f.txt", "w")
small=[]

for key, counts in bigram_model_counter.most_common(1000):
    
	small.append(key.encode("utf-8"))
for word in small:
	outfile.write(str(word)+" ") #Printing bigrams
	
