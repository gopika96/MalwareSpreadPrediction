from urllib.request import urlopen
from bs4 import BeautifulSoup
import re 
import csv


#----------Base URL ----------------

url_arr=["https://www.schneier.com/essays/"]
times =200

#------------creating a list of article specific URLs------------------

while (times > 0):
	if url_arr[-1]=='https://www.schneier.com/cgi-bin/mt/mt-search.cgi?search=&IncludeBlogs=5&blog_id=5&archive_type=Index&template_id=354&limit=10&page=2':
		html = urlopen(url_arr[-2])
	else:
		html = urlopen(url_arr[-1])
	bsObj = BeautifulSoup(html,"lxml")

	for link in bsObj.find("div",{"id":"content"}).findAll("a", href =re.compile('https://www.schneier.com/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')):
		if 'href' in link.attrs:
			if link.attrs['href'] not in url_arr:
				new_url = link.attrs['href']
				print(new_url)
				url_arr.append(new_url)

	times = times -1

#---------------------Visiting each URL and extracting the data----------------


final_ar =[]
for ele in url_arr:
	result = re.match('https://www.schneier.com/essays/archives/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',ele)
	if result:
		article_ls=[]
		htm = urlopen(ele)
		soup = BeautifulSoup(htm,"lxml")
		#title=soup.find('h3', class_="subtitle");
		#sub_title =title.get_text()
		essay=soup.find('div', class_="article");
		article=essay.get_text()
		
		date=soup.find('li', class_="pubdate");
		da = date.get_text()
		#article_ls.append(sub_title)
		article_ls= [article, da]
		final_ar.append(article_ls)
		

import pandas as pd
df = pd.DataFrame()
df = df.append(final_ar)
df.to_csv('sch_blog.csv',encoding='utf-8')
	

